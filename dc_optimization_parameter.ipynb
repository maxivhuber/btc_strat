{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19bfe135",
   "metadata": {},
   "source": [
    "This notebook applies the directional-change (DC) framework to multi-asset price data using two optimization functions:  \n",
    "- `event_density_score`, which measures how closely the observed DC event density matches a target density $d_{\\text{target}}$, and  \n",
    "- `up_down_asymmetry`, which quantifies the strength of upward and downward trend continuation via normalized overshoot scores.\n",
    "\n",
    "The analysis proceeds in three stages:  \n",
    "1. **Event-density evaluation**: For a selected asset, volatility $\\sigma = \\operatorname{std}(\\ln(P_t / P_{t-1}))$ is computed, and the event-density score is evaluated over a grid of threshold multipliers $k \\in [8, 52]$, with $\\theta = k \\cdot \\sigma$.  \n",
    "2. **Global squash calibration**: Using all assets, empirical overshoot magnitudes  \n",
    "   $$\n",
    "   r_i = \\frac{(P_{\\text{ext},i} - P_{\\text{ext},i-1}) / P_{\\text{ext},i-1}}{\\theta}\n",
    "   $$  \n",
    "   are collected, and a universal squash parameter $s^*$ is estimated via hierarchical bootstrap for the mapping $\\mu(r) = 1 - e^{-r/s}$.  \n",
    "3. **Asymmetry visualization**: For the selected asset, the asymmetry scores $\\mu_{\\text{up}}$ and $\\mu_{\\text{down}}$ are computed across the same $k$-grid using the calibrated $s^*$, and visualized to assess trend strength as a function of threshold scale.\n",
    "\n",
    "The output is a **calibrated squash parameter** $s^*$ and **diagnostic plots** showing how event density and asymmetry vary with $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data loading\n",
    "from core.data import load_all_klines\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your DC framework\n",
    "from core.dc import (\n",
    "    compute_directional_change_events,\n",
    "    attach_OSV_EXT_to_runs,\n",
    ")\n",
    "from core.opt import event_density_score, up_down_asymmetry\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "SELECTED_COIN = \"BTCUSDT\"  # Change this to analyze different coins\n",
    "\n",
    "# Load 4+ years of hourly price data for all available symbols\n",
    "df = load_all_klines(\n",
    "    root=\"data/data/spot/monthly/klines/\",\n",
    "    interval=\"1h\",\n",
    "    range_folder=\"2017-01-01_2025-10-08\",\n",
    "    min_years=4,\n",
    ")\n",
    "\n",
    "print(f\"Loaded data for {df.index.get_level_values('Symbol').nunique()} assets\")\n",
    "print(\n",
    "    f\"Date range: {df.index.get_level_values('Open Time').min()} to {df.index.get_level_values('Open Time').max()}\"\n",
    ")\n",
    "\n",
    "# Get prices for selected coin\n",
    "prices = df.loc[SELECTED_COIN].Close.dropna().to_numpy()\n",
    "print(f\"Analyzing {SELECTED_COIN} with {len(prices)} price points\")\n",
    "print(\"===========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b96b79",
   "metadata": {},
   "source": [
    "## Event Density\n",
    "This code visualizes how well the **observed directional-change (DC) event density** matches a **target density** $d_{\\text{target}} = 0.002$ (≈1 event per 500 samples) as a function of a **universal scaling factor** $k$.\n",
    "\n",
    "For a given asset price series $P_t$, it first computes the **volatility** as the standard deviation of log returns:\n",
    "$$\n",
    "\\sigma = \\operatorname{std}\\big(\\ln(P_t / P_{t-1})\\big),\n",
    "$$\n",
    "which is dimensionless and reflects the asset’s typical % move.\n",
    "\n",
    "The DC threshold is then set **adaptively** as:\n",
    "$$\n",
    "\\theta = k \\cdot \\sigma,\n",
    "$$\n",
    "so that $k$ controls how many “typical moves” are required to trigger a directional change. The loop tests $k \\in [8, 52]$, corresponding to thresholds from $8\\sigma$ to $52\\sigma$.\n",
    "\n",
    "For each $k$, the DC pipeline detects events, and the **event density score** is computed as:\n",
    "$$\n",
    "s(d) = \\exp\\!\\Big(-\\beta \\, \\big|\\ln(d / d_{\\text{target}})\\big|^\\alpha\\Big), \\quad \\text{where} \\quad d = \\frac{N}{T},\n",
    "$$\n",
    "with $N$ = number of runs, $T$ = total samples, $\\alpha = 2$, $\\beta = 1.5$.  \n",
    "This score is **dimensionless**, peaks at 1 when $d = d_{\\text{target}}$, and symmetrically penalizes under- or over-detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute volatility (log returns std) ---\n",
    "log_returns = np.diff(np.log(prices))\n",
    "sigma = np.std(log_returns)\n",
    "print(f\"{SELECTED_COIN} volatility (σ): {sigma:.4f} (~{sigma * 100:.2f}%)\")\n",
    "\n",
    "# --- Define k range (universal scaling factor) ---\n",
    "k_values_density = np.linspace(8, 52, 88)  # Test thresholds from 8σ to 52σ\n",
    "\n",
    "# Target density (1 event per ~500 samples)\n",
    "d_target = 0.002\n",
    "\n",
    "# --- Initialize storage ---\n",
    "score_vals, counts, thetas_used = [], [], []\n",
    "\n",
    "# --- Loop over k values ---\n",
    "for k in k_values_density:\n",
    "    theta = k * sigma  # Adaptive threshold\n",
    "\n",
    "    # Run DC pipeline\n",
    "    events, runs = compute_directional_change_events(prices, theta)\n",
    "    runs = attach_OSV_EXT_to_runs(runs, theta)\n",
    "\n",
    "    # Compute scores\n",
    "    penalty = event_density_score(prices, events, d_target=d_target, alpha=2, beta=1.5)\n",
    "    score_vals.append(penalty)\n",
    "    counts.append(len(runs))\n",
    "    thetas_used.append(theta)\n",
    "\n",
    "# Convert to arrays\n",
    "score_vals = np.array(score_vals)\n",
    "counts = np.array(counts)\n",
    "thetas_used = np.array(thetas_used)\n",
    "\n",
    "# --- Compute derived quantities ---\n",
    "T = len(prices)\n",
    "densities = counts / T\n",
    "ratio = densities / d_target\n",
    "log_ratio = np.log10(ratio)\n",
    "\n",
    "# Sort for smooth curve\n",
    "sorted_idx = np.argsort(log_ratio)\n",
    "log_ratio_sorted = log_ratio[sorted_idx]\n",
    "penalty_sorted = score_vals[sorted_idx]\n",
    "\n",
    "# --- Create interactive plot ---\n",
    "fig = go.Figure()\n",
    "\n",
    "# Primary trace: Penalty vs k\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=k_values_density,\n",
    "        y=score_vals,\n",
    "        name=\"Penalty (vs k)\",\n",
    "        mode=\"lines+markers\",\n",
    "        marker=dict(color=\"blue\"),\n",
    "        line=dict(color=\"blue\"),\n",
    "        hovertemplate=\"k=%{x:.2f}<br>Penalty=%{y:.4f}<br>θ=%{text:.3f}%\",\n",
    "        text=np.round(thetas_used * 100, 3),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Secondary trace: Penalty vs log10(d/d_target)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=log_ratio_sorted,\n",
    "        y=penalty_sorted,\n",
    "        name=\"Penalty (vs log₁₀(d/dₜ))\",\n",
    "        mode=\"lines+markers\",\n",
    "        marker=dict(color=\"green\", symbol=\"circle\"),\n",
    "        line=dict(color=\"green\", dash=\"dot\"),\n",
    "        hovertemplate=\"log₁₀(d/dₜ)=%{x:.3f}<br>Penalty=%{y:.4f}\",\n",
    "        xaxis=\"x2\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Right y-axis: Event count\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=k_values_density,\n",
    "        y=counts,\n",
    "        name=\"Event count\",\n",
    "        mode=\"lines+markers\",\n",
    "        marker=dict(color=\"red\", symbol=\"square\"),\n",
    "        line=dict(color=\"red\", dash=\"dash\"),\n",
    "        yaxis=\"y2\",\n",
    "        hovertemplate=\"k=%{x:.2f}<br>Runs=%{y}\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Layout\n",
    "fig.update_layout(\n",
    "    title=f\"Event-count penalty vs universal scaling factor k ({SELECTED_COIN})\",\n",
    "    xaxis=dict(title=\"Universal scaling factor k\", showgrid=True),\n",
    "    xaxis2=dict(\n",
    "        title=\"log₁₀(observed density / target density)\",\n",
    "        showgrid=False,\n",
    "        overlaying=\"x\",\n",
    "        side=\"top\",\n",
    "        anchor=\"y\",\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=dict(text=\"Event-count penalty\", font=dict(color=\"blue\")),\n",
    "        tickfont=dict(color=\"blue\"),\n",
    "        range=[0, 1],\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title=dict(text=\"Number of runs\", font=dict(color=\"red\")),\n",
    "        tickfont=dict(color=\"red\"),\n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "    ),\n",
    "    legend=dict(x=0.02, y=0.98, bgcolor=\"rgba(0,0,0,0)\"),\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae9565d",
   "metadata": {},
   "source": [
    "The plot shows:\n",
    "- **Blue curve**: penalty score vs. $k$ (primary x-axis)\n",
    "- **Green curve**: same score vs. $\\log_{10}(d / d_{\\text{target}})$ (top x-axis), illustrating the symmetric log-ratio penalty\n",
    "- **Red curve**: number of detected runs (right y-axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac084f",
   "metadata": {},
   "source": [
    "## Squash Parameter Calibration (Asymmetry Score)\n",
    "\n",
    "This code calibrates a universal **squash parameter** $ s $ for the normalized overshoot mapping  \n",
    "$$\n",
    "\\mu(r) = 1 - e^{-r / s},\n",
    "$$  \n",
    "using a hierarchical bootstrap across multiple assets to ensure cross-asset robustness.\n",
    "\n",
    "For each asset, it first computes the **volatility** as the standard deviation of log returns:  \n",
    "$$\n",
    "\\sigma = \\operatorname{std}\\big(\\ln(P_t / P_{t-1})\\big),\n",
    "$$  \n",
    "which is dimensionless and characterizes the asset’s typical relative move.\n",
    "\n",
    "Across a grid of threshold multipliers $ k \\in [8, 52] $, it sets the directional-change threshold as $ \\theta = k \\cdot \\sigma $, runs the DC detection pipeline, and collects the absolute values of the **dimensionless overshoots**:  \n",
    "$$\n",
    "r_i = \\frac{(P_{\\text{ext},i} - P_{\\text{ext},i-1}) / P_{\\text{ext},i-1}}{\\theta} = \\texttt{OSV\\_EXT}_i.\n",
    "$$\n",
    "\n",
    "Only assets with at least 50 valid overshoot samples are retained. For each such asset $ c $, the median overshoot $ \\tilde{r}_c = \\operatorname{median}(|r_i|) $ is computed. The baseline squash estimate is the median of these medians across all assets.\n",
    "\n",
    "To quantify uncertainty, a bootstrap procedure is performed: in each of $ m = 50 $ iterations, a random sample of 70% of assets is drawn with replacement, and the median of their medians is recorded, yielding a distribution $ \\{s^{(j)}\\}_{j=1}^{m} $.\n",
    "\n",
    "The final **global squash parameter** is set to the median of this bootstrap distribution:  \n",
    "$$\n",
    "s^* = \\operatorname{median}\\big(\\{s^{(j)}\\}_{j=1}^{m}\\big),\n",
    "$$  \n",
    "with a 90% confidence interval derived from the 5th and 95th percentiles. This $ s^* $ ensures the asymmetry score $ \\mu(r) $ saturates at a rate consistent with empirical overshoot behavior across the asset universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a37dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "sample_frac = 0.7\n",
    "m_bootstrap = 50\n",
    "k_values_bootstrap = np.linspace(8, 52, 88)  # Test thresholds from 8σ to 52σ\n",
    "all_coins = df.index.get_level_values(\"Symbol\").unique().to_list()\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# --- Precompute volatility per coin ---\n",
    "volatilities = {}\n",
    "valid_coins = []\n",
    "\n",
    "for coin in all_coins:\n",
    "    coin_prices = df.loc[coin].Close.dropna().to_numpy()\n",
    "    if len(coin_prices) < 10:\n",
    "        continue\n",
    "    log_ret = np.diff(np.log(coin_prices))\n",
    "    sigma_coin = np.std(log_ret)\n",
    "    if sigma_coin > 0:\n",
    "        volatilities[coin] = sigma_coin\n",
    "        valid_coins.append(coin)\n",
    "\n",
    "if not valid_coins:\n",
    "    raise ValueError(\"No valid coins with positive volatility\")\n",
    "\n",
    "print(f\"Valid coins with positive volatility: {len(valid_coins)}\")\n",
    "\n",
    "\n",
    "# --- Helper function to collect |OSV_EXT| values ---\n",
    "def collect_osv_abs(\n",
    "    prices: np.ndarray, sigma: float, k_values: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    ratios = []\n",
    "    for k in k_values:\n",
    "        theta = k * sigma\n",
    "        events, runs = compute_directional_change_events(prices, theta)\n",
    "        runs = attach_OSV_EXT_to_runs(runs, theta)\n",
    "        osv_vals = [r[\"OSV_EXT\"] for r in runs if r.get(\"OSV_EXT\") is not None]\n",
    "        osv_vals = np.array(osv_vals, dtype=float)\n",
    "        osv_vals = osv_vals[~np.isnan(osv_vals)]\n",
    "        if osv_vals.size == 0:\n",
    "            continue\n",
    "        ratios.extend(np.abs(osv_vals))\n",
    "    return np.array(ratios)\n",
    "\n",
    "\n",
    "# --- Collect |OSV_EXT| per coin ---\n",
    "ratios_per_coin = {}\n",
    "for coin in valid_coins:\n",
    "    coin_prices = df.loc[coin].Close.dropna().to_numpy()\n",
    "    ratios = collect_osv_abs(coin_prices, volatilities[coin], k_values_bootstrap)\n",
    "    if ratios.size >= 50:  # Require minimum samples\n",
    "        ratios_per_coin[coin] = ratios\n",
    "\n",
    "available_coins = list(ratios_per_coin.keys())\n",
    "n_total = len(available_coins)\n",
    "if n_total == 0:\n",
    "    raise ValueError(\"No coins with sufficient overshoot data\")\n",
    "\n",
    "print(f\"Coins with sufficient overshoot data: {n_total}\")\n",
    "\n",
    "# --- Baseline: median of coin medians ---\n",
    "coin_medians = {c: np.median(r) for c, r in ratios_per_coin.items()}\n",
    "baseline = np.median(list(coin_medians.values()))\n",
    "print(f\"\\nBaseline median of |OSV_EXT|: {baseline:.2f} from {n_total} coins\")\n",
    "\n",
    "# --- Bootstrap hierarchical medians ---\n",
    "boot_squash = []\n",
    "for i in range(m_bootstrap):\n",
    "    n_sample = max(1, int(sample_frac * n_total))\n",
    "    sample = rng.choice(available_coins, size=n_sample, replace=True)\n",
    "    sample_medians = [np.median(ratios_per_coin[c]) for c in sample]\n",
    "    boot_squash.append(np.median(sample_medians))\n",
    "boot_squash = np.array(boot_squash)\n",
    "\n",
    "# --- Bootstrap diagnostics ---\n",
    "print(\"\\n--- Bootstrap stability ---\")\n",
    "mean_s, std_s = boot_squash.mean(), boot_squash.std()\n",
    "cv = std_s / mean_s\n",
    "print(f\"Mean squash  : {mean_s:.2f}\")\n",
    "print(f\"Std deviation: {std_s:.2f}\")\n",
    "print(f\"Coeff. of variation: {cv:.3f}\")\n",
    "\n",
    "# Plot 1: Histogram of bootstrap squash estimates\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(boot_squash, bins=12, color=\"cornflowerblue\", edgecolor=\"k\", alpha=0.8)\n",
    "plt.axvline(mean_s, color=\"red\", linestyle=\"--\", label=f\"Mean = {mean_s:.2f}\")\n",
    "plt.title(\"Bootstrap Distribution of Squash Estimates\")\n",
    "plt.xlabel(\"Estimated Squash Parameter\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- μ(r) stability curves ---\n",
    "combined = np.concatenate(list(ratios_per_coin.values()))\n",
    "test_r = np.linspace(np.percentile(combined, 5), np.percentile(combined, 95), 50)\n",
    "\n",
    "# Plot 2: μ(r) curves for all bootstrap samples\n",
    "plt.figure(figsize=(10, 6))\n",
    "for s in boot_squash:\n",
    "    plt.plot(test_r, 1 - np.exp(-test_r / s), color=\"gray\", alpha=0.3, lw=0.8)\n",
    "\n",
    "mu_curves = np.array([1 - np.exp(-test_r / s) for s in boot_squash])\n",
    "mu_mean = mu_curves.mean(axis=0)\n",
    "mu_std = mu_curves.std(axis=0)\n",
    "\n",
    "plt.plot(test_r, mu_mean, \"k\", lw=2.5, label=\"Mean μ Curve\")\n",
    "plt.fill_between(\n",
    "    test_r,\n",
    "    mu_mean - mu_std,\n",
    "    mu_mean + mu_std,\n",
    "    color=\"lightgray\",\n",
    "    alpha=0.6,\n",
    "    label=\"±1σ Band\",\n",
    ")\n",
    "plt.title(\"Stability of μ(r) = 1 - exp(-r/s) Across Bootstrap Samples\")\n",
    "plt.xlabel(\"|OSV_EXT| (Overshoot in Units of Theta)\")\n",
    "plt.ylabel(\"Normalized Overshoot Score μ\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Final global squash estimate ---\n",
    "global_squash = np.median(boot_squash)\n",
    "ci_low, ci_high = np.percentile(boot_squash, [5, 95])\n",
    "print(f\"\\n✅ Recommended global squash: {global_squash:.2f}\")\n",
    "print(f\"   90% bootstrap CI: [{ci_low:.2f}, {ci_high:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83078ee5",
   "metadata": {},
   "source": [
    "## Asymmetry Score\n",
    "This code evaluates how **strongly upward and downward price trends overshoot** the directional-change (DC) threshold across a range of **volatility-scaled thresholds** $ \\theta = k \\cdot \\sigma $, and identifies the optimal $ k $ that maximizes **balanced trend strength**.\n",
    "\n",
    "For each candidate $ k \\in [8, 52] $, it:\n",
    "1. Sets the DC threshold as $ \\theta = k \\cdot \\sigma $, where $ \\sigma $ is the asset’s log-return volatility.\n",
    "2. Detects DC events and computes the **dimensionless overshoot** for each run:\n",
    "   $$\n",
    "   r_i = \\frac{(P_{\\text{ext},i} - P_{\\text{ext},i-1}) / P_{\\text{ext},i-1}}{\\theta},\n",
    "   $$\n",
    "   which measures how many **threshold-sized moves** occurred between consecutive extrema.\n",
    "3. Computes **normalized asymmetry scores**:\n",
    "   $$\n",
    "   \\mu_{\\text{up}} = 1 - e^{-\\bar{r}_{\\text{up}} / s}, \\quad\n",
    "   \\mu_{\\text{down}} = 1 - e^{-\\bar{r}_{\\text{down}} / s},\n",
    "   $$\n",
    "   where $ \\bar{r}_{\\text{up}}, \\bar{r}_{\\text{down}} $ are the mean overshoots for upward and downward runs, and $ s = \\texttt{SQUASH\\_VALUE} $ controls saturation (higher $ s $ → slower approach to 1).\n",
    "\n",
    "The **asymmetry score** is the average $ (\\mu_{\\text{up}} + \\mu_{\\text{down}})/2 \\in [0,1] $, peaking when **both directions exhibit strong, sustained moves** beyond the DC trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81787d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the calibrated squash value from bootstrap\n",
    "SQUASH_VALUE = global_squash\n",
    "\n",
    "# Reuse volatility computed earlier\n",
    "sigma_coin = sigma  # From Cell 3\n",
    "\n",
    "# --- Define k range for asymmetry analysis ---\n",
    "k_values_asym = np.linspace(8, 52, 88)\n",
    "\n",
    "# --- Run optimization loop ---\n",
    "asym_scores = []\n",
    "run_counts = []\n",
    "mu_up_vals = []\n",
    "mu_down_vals = []\n",
    "\n",
    "for k in k_values_asym:\n",
    "    theta = k * sigma_coin\n",
    "\n",
    "    # Run DC pipeline\n",
    "    events, runs = compute_directional_change_events(prices, theta)\n",
    "    runs = attach_OSV_EXT_to_runs(runs, theta)\n",
    "\n",
    "    # Compute asymmetry\n",
    "    mu_up, mu_down = up_down_asymmetry(runs, theta, squash=SQUASH_VALUE)\n",
    "    score = (mu_up + mu_down) / 2.0\n",
    "\n",
    "    asym_scores.append(score)\n",
    "    run_counts.append(len(runs))\n",
    "    mu_up_vals.append(mu_up)\n",
    "    mu_down_vals.append(mu_down)\n",
    "\n",
    "# --- Create interactive plot ---\n",
    "fig = go.Figure()\n",
    "\n",
    "# Primary trace: asymmetry score\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=k_values_asym,\n",
    "        y=asym_scores,\n",
    "        name=\"Up–Down Asymmetry Score (avg)\",\n",
    "        mode=\"lines+markers\",\n",
    "        marker=dict(color=\"dodgerblue\"),\n",
    "        line=dict(color=\"dodgerblue\"),\n",
    "        hovertemplate=\"k = %{x:.1f}σ<br>Asymmetry Score = %{y:.4f}<extra></extra>\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Upward component\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=k_values_asym,\n",
    "        y=mu_up_vals,\n",
    "        name=\"μ↑ (Upward)\",\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"green\", dash=\"dot\"),\n",
    "        hovertemplate=\"k = %{x:.1f}σ<br>μ↑ = %{y:.4f}<extra></extra>\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Downward component\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=k_values_asym,\n",
    "        y=mu_down_vals,\n",
    "        name=\"μ↓ (Downward)\",\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"purple\", dash=\"dot\"),\n",
    "        hovertemplate=\"k = %{x:.1f}σ<br>μ↓ = %{y:.4f}<extra></extra>\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Secondary trace: number of runs\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=k_values_asym,\n",
    "        y=run_counts,\n",
    "        name=\"Number of Events\",\n",
    "        mode=\"lines+markers\",\n",
    "        marker=dict(color=\"red\", symbol=\"square\"),\n",
    "        line=dict(color=\"red\", dash=\"dash\"),\n",
    "        yaxis=\"y2\",\n",
    "        hovertemplate=\"k = %{x:.1f}σ<br>Events = %{y}<extra></extra>\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Layout\n",
    "fig.update_layout(\n",
    "    title=f\"{SELECTED_COIN}: Normalized Up–Down Asymmetry Score vs. Volatility-Scaled Threshold (k)<br><sub>Squash = {SQUASH_VALUE:.2f}</sub>\",\n",
    "    xaxis=dict(\n",
    "        title=\"Threshold Multiplier k (θ = k · σ)\",\n",
    "        tickmode=\"linear\",\n",
    "        dtick=5,\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=dict(text=\"Asymmetry Score [0–1]\", font=dict(color=\"dodgerblue\")),\n",
    "        tickfont=dict(color=\"dodgerblue\"),\n",
    "        range=[0, 1],\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title=dict(text=\"Number of Runs\", font=dict(color=\"red\")),\n",
    "        tickfont=dict(color=\"red\"),\n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "    ),\n",
    "    legend=dict(x=0.02, y=0.98, bgcolor=\"rgba(0,0,0,0)\"),\n",
    "    template=\"plotly_white\",\n",
    "    width=900,\n",
    "    height=500,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Find optimal k for selected coin based on asymmetry score\n",
    "optimal_idx = np.argmax(asym_scores)\n",
    "optimal_k = k_values_asym[optimal_idx]\n",
    "optimal_score = asym_scores[optimal_idx]\n",
    "optimal_runs = run_counts[optimal_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0390c207",
   "metadata": {},
   "source": [
    "The plot shows:\n",
    "- **Blue curve**: overall asymmetry score vs. $ k $\n",
    "- **Green/purple curves**: individual $ \\mu_{\\text{up}} $ and $ \\mu_{\\text{down}} $ components\n",
    "- **Red curve**: number of detected runs (right axis)\n",
    "\n",
    "The optimal $ k $ (marked by peak score) balances **sufficient event count** with **maximal trend continuation strength**, providing a robust, scale-invariant parameter for cross-asset DC analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FINAL PARAMETER RECOMMENDATIONS ===\")\n",
    "print(f\"Global squash parameter: {global_squash:.2f}\")\n",
    "print(\"\")\n",
    "print(f\"For {SELECTED_COIN}:\")\n",
    "print(f\"  Optimal k: {optimal_k:.1f}\")\n",
    "print(\n",
    "    f\"  Optimal theta: {optimal_k * sigma_coin:.4f} ({optimal_k * sigma_coin * 100:.2f}%)\"\n",
    ")\n",
    "print(f\"  Expected runs: {optimal_runs}\")\n",
    "print(f\"  Asymmetry score: {optimal_score:.4f}\")\n",
    "print(\"\")\n",
    "print(\"For other assets, use the same global squash parameter\")\n",
    "print(\"and compute theta = optimal_k * asset_volatility\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "btc-strat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
